{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C8XY66/GIN/blob/main/GIN_ModelCheckpoint_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9crjFcNyO9lN"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6ERJo-3U1Uq"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMXW6FKbU23u"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/optuna/optuna.git\n",
        "!pip install optuna-dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh5Jy70SU4Vm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import datetime\n",
        "import pytz\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data.lightning import LightningDataset\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GIN, MLP, global_add_pool\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "from optuna.visualization.matplotlib import plot_contour, plot_edf, plot_intermediate_values, plot_optimization_history, plot_parallel_coordinate, plot_param_importances, plot_slice\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "MAIN_DIR = \"/content/gdrive/My Drive/ColabNotebooks/\" \n",
        "PARENT_DIR = None\n",
        "\n",
        "\n",
        "import logging\n",
        "#logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
        "#logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
        "#logging.getLogger(\"lightning\").setLevel(logging.CRITICAL)\n",
        "#logging.getLogger('lightning').setLevel(0)\n",
        "#logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"pytorch_lightning.callbacks.early_stopping\").setLevel(logging.WARNING)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atTqw8aphQQp",
        "outputId": "f3539c7f-3178-4a69-88d7-f85b66da64ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Check for CUDA system support and use GPU if available otherwise run on CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # device = Context-manager that changes the selected device\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lTyVGyyJrGDW"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "DATASET_NAME = 'NCI1'\n",
        "#num_layers = 5\n",
        "lr = 0.01 \n",
        "EPOCHS = 20 #final = 1000\n",
        "SEED = 42\n",
        "N_SPLITS = 3\n",
        "REP = 2\n",
        "\n",
        "LOAD_FROM_CHECKPOINT = True\n",
        "RUN = \"NCI1_reps_2_folds_3_epochs_20_2023-04-20_14-17\"\n",
        "CHECKPOINT_PATH = os.path.join(MAIN_DIR, \"logs\", RUN, \"checkpoints\")\n",
        "\n",
        "#CHECKPOINT_PATH = os.path.join(MAIN_DIR, \"logs/NCI1_reps_2_folds_3_epochs_20_2023-04-20_14-17/checkpoints/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(pl.LightningModule):  \n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 hidden_channels: int, dropout, num_layers=5):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.gnn = GIN(in_channels, hidden_channels, num_layers,\n",
        "                       dropout=dropout, jk='cat')\n",
        "\n",
        "        self.classifier = MLP([hidden_channels, hidden_channels, out_channels],\n",
        "                              norm=\"batch_norm\", dropout=dropout)\n",
        "\n",
        "        self.train_acc = Accuracy(task='multiclass', num_classes=out_channels)\n",
        "        self.val_acc = Accuracy(task='multiclass', num_classes=out_channels)\n",
        "        self.test_acc = Accuracy(task='multiclass', num_classes=out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.gnn(x, edge_index)\n",
        "        x = global_add_pool(x, batch)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, data, batch_idx):\n",
        "        y_hat = self(data.x, data.edge_index, data.batch)\n",
        "        loss = F.cross_entropy(y_hat, data.y)\n",
        "        self.train_acc(y_hat.softmax(dim=-1), data.y)\n",
        "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('train_acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, data, batch_idx):\n",
        "        y_hat = self(data.x, data.edge_index, data.batch)\n",
        "        loss = F.cross_entropy(y_hat, data.y)\n",
        "        self.val_acc(y_hat.softmax(dim=-1), data.y)\n",
        "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('val_acc', self.val_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "    def test_step(self, data, batch_idx):\n",
        "        y_hat = self(data.x, data.edge_index, data.batch)\n",
        "        loss = F.cross_entropy(y_hat, data.y)\n",
        "        self.test_acc(y_hat.softmax(dim=-1), data.y)\n",
        "        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('test_acc', self.test_acc, prog_bar=True, on_step=False,on_epoch=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.01)\n",
        "    \n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        checkpoint[\"init_args\"] = self.hparams"
      ],
      "metadata": {
        "id": "pgN_QJgyhoNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKE31eyglQ9p"
      },
      "outputs": [],
      "source": [
        "class GraphDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, dataset_name, n_splits=10, fold=0):\n",
        "        super().__init__()\n",
        "        self.dataset_name = dataset_name\n",
        "        self.n_splits = n_splits\n",
        "        self.fold = fold\n",
        "\n",
        "    def prepare_data(self):    \n",
        "        self.dataset = TUDataset(root='data/TUDataset', name=self.dataset_name)\n",
        "        self.dataset = self.dataset[:1000] #for quick experiments\n",
        "        self.skf = StratifiedKFold(n_splits=self.n_splits)\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None, fold: int = 0, batch_size: int = 32):\n",
        "        self.fold = fold\n",
        "        self.batch_size = batch_size\n",
        "        y = [data.y.item() for data in self.dataset]\n",
        "\n",
        "        train_indices, test_indices = list(self.skf.split(torch.zeros(len(y)), y))[self.fold]\n",
        "        train_dataset = self.dataset[train_indices]\n",
        "        \n",
        "        num_val = int(len(train_dataset) * 0.1)\n",
        "        num_train = len(train_dataset) - num_val\n",
        "        \n",
        "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(train_dataset, [num_train, num_val])\n",
        "        self.test_dataset = self.dataset[test_indices]\n",
        "      \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    @property\n",
        "    def num_node_features(self):\n",
        "        return self.dataset.num_node_features\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return self.dataset.num_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_log_dir(repetition_index, fold_index, extra_info=''):\n",
        "    global PARENT_DIR\n",
        "    \n",
        "    # Current timestamp\n",
        "    now = datetime.datetime.now(pytz.timezone('Europe/Zurich')).strftime(\"%Y-%m-%d_%H-%M\")\n",
        "\n",
        "    # Parent directory\n",
        "    parent_dir_info = f\"{DATASET_NAME}_reps_{REP}_folds_{N_SPLITS}_epochs_{EPOCHS}\"\n",
        "\n",
        "    if PARENT_DIR is None:\n",
        "        PARENT_DIR = f\"{MAIN_DIR}logs/{parent_dir_info}_{now}\"\n",
        "        if not os.path.exists(PARENT_DIR):\n",
        "            os.makedirs(PARENT_DIR)\n",
        "\n",
        "    # Subdirectory for the specific repetition and fold\n",
        "    if repetition_index is not None and fold_index is not None:\n",
        "        sub_dir = f\"{PARENT_DIR}/rep_{repetition_index}_fold_{fold_index}{extra_info}\"\n",
        "        if not os.path.exists(sub_dir):\n",
        "            os.makedirs(sub_dir)\n",
        "    else:\n",
        "        sub_dir = PARENT_DIR\n",
        "    \n",
        "    return sub_dir"
      ],
      "metadata": {
        "id": "po9ze2H0yJmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_trainer(log_dir, epochs, pruning_callback=None, checkpoint_callback=None):\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=10, verbose=True)\n",
        "  \n",
        "    callbacks = [early_stopping]\n",
        "    if pruning_callback is not None:\n",
        "        callbacks.append(pruning_callback)\n",
        "    if checkpoint_callback is not None:\n",
        "        callbacks.append(checkpoint_callback)\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = pl.Trainer(\n",
        "        callbacks=callbacks,\n",
        "        max_epochs=epochs,\n",
        "        log_every_n_steps=5,\n",
        "        logger=TensorBoardLogger(save_dir=log_dir),\n",
        "        enable_progress_bar=False,\n",
        "        enable_model_summary=False,\n",
        "    )\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "8h1J2xdqMzV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmWBXCATF5VS"
      },
      "outputs": [],
      "source": [
        "def objective(trial, datamodule, epochs, repetition_index, fold_index): \n",
        "\n",
        "    # Optimise hyperparameters\n",
        "    hidden_channels = trial.suggest_categorical('hidden_channels', [16, 32])\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 128])\n",
        "    dropout = trial.suggest_categorical('dropout', [0.0, 0.5])\n",
        "\n",
        "    # Model and DataModule\n",
        "    datamodule.setup(fold=0, batch_size=batch_size)\n",
        "    model = GNNModel(in_channels=datamodule.num_node_features, out_channels=datamodule.num_classes, hidden_channels=hidden_channels, dropout=dropout)\n",
        "\n",
        "    # Training\n",
        "    pruning_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_acc\") #from optuna-pl-integration\n",
        "    \n",
        "    log_dir = create_log_dir(repetition_index, fold_index, extra_info='_selection')\n",
        "    trainer = create_trainer(log_dir, epochs=epochs, pruning_callback=pruning_callback, checkpoint_callback=None)\n",
        "    \n",
        "    hyperparameters = dict(hidden_channels=hidden_channels, batch_size=batch_size, epochs=epochs, dropout=dropout)\n",
        "    trainer.logger.log_hyperparams(hyperparameters)    \n",
        "    \n",
        "    trainer.fit(model, datamodule=datamodule)\n",
        "\n",
        "    return trainer.callback_metrics['val_acc'].item()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhW7-XpMDGLv"
      },
      "outputs": [],
      "source": [
        "def retrain_and_evaluate(model, datamodule, epochs, repetition_index, fold_index, load_from_checkpoint=False):\n",
        "    \n",
        "    log_dir = create_log_dir(repetition_index, fold_index, extra_info='_assessment')\n",
        "\n",
        "    if not load_from_checkpoint:      \n",
        "        # ModelCheckpoint\n",
        "        checkpoint_callback = ModelCheckpoint(monitor=\"val_acc\",\n",
        "                                              mode=\"max\",\n",
        "                                              dirpath=f\"{PARENT_DIR}/checkpoints\",\n",
        "                                              filename=f\"best_model_rep_{repetition_index}_fold_{fold_index}_assessment\")\n",
        "        \n",
        "        trainer = create_trainer(log_dir, epochs=epochs, pruning_callback=None, checkpoint_callback=checkpoint_callback)\n",
        "  \n",
        "        trainer.fit(model, datamodule=datamodule)\n",
        "\n",
        "    else:\n",
        "        trainer = create_trainer(log_dir, epochs=epochs, pruning_callback=None, checkpoint_callback=None)\n",
        "  \n",
        "    trainer.test(model, datamodule=datamodule)\n",
        "\n",
        "    return trainer.callback_metrics['test_acc'].item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_last_checkpoint(checkpoint_path):\n",
        "    # find most recent checkpoint in the folder provided by checkpoint_path\n",
        "    checkpoint_files = os.listdir(checkpoint_path)\n",
        "    checkpoint_files = sorted(checkpoint_files, key=lambda x: os.path.getmtime(os.path.join(checkpoint_path, x)), reverse=True)\n",
        "    #checkpoints = sorted(os.listdir(checkpoint_path), reverse=True)\n",
        "\n",
        "    if not checkpoint_files:\n",
        "        raise FileNotFoundError(f\"No checkpoint files found in {checkpoint_path}\")\n",
        "\n",
        "    last_checkpoint = os.path.join(checkpoint_path, checkpoint_files[0])\n",
        "\n",
        "    # Extract the repetition and fold numbers from the filename\n",
        "    pattern = r\"best_model_rep_(\\d+)_fold_(\\d+)_assessment\"\n",
        "    match = re.search(pattern, last_checkpoint)\n",
        "\n",
        "    if match:\n",
        "        starting_rep = int(match.group(1))\n",
        "        starting_fold = int(match.group(2))\n",
        "    else:\n",
        "        raise ValueError(\"Could not extract repetition and fold numbers from the checkpoint filename\")\n",
        "\n",
        "    return last_checkpoint, starting_rep, starting_fold"
      ],
      "metadata": {
        "id": "oeJ2jsOYBdKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__': \n",
        "\n",
        "    datamodule = GraphDataModule(dataset_name=DATASET_NAME)\n",
        "  \n",
        "    overall_performances = []\n",
        "    starting_rep, starting_fold = 0, 0\n",
        "\n",
        "    if LOAD_FROM_CHECKPOINT:\n",
        "        checkpoint_path, starting_rep, starting_fold = load_last_checkpoint(CHECKPOINT_PATH)\n",
        "        checkpoint = torch.load(checkpoint_path)  # Load the checkpoint dictionary from the file\n",
        "        init_args = checkpoint[\"init_args\"]  # Access the saved initialization parameters\n",
        "        model = GNNModel(**init_args)  # Initialize the model using the saved parameters\n",
        "        \n",
        "    for r in range(starting_rep, REP):\n",
        "        datamodule.prepare_data()\n",
        "        fold_performances = []\n",
        "        for fold in range(starting_fold if r == starting_rep else 0, N_SPLITS):\n",
        "            if LOAD_FROM_CHECKPOINT and r == starting_rep and fold == starting_fold:\n",
        "                test_acc = retrain_and_evaluate(model, datamodule, EPOCHS, r, fold, load_from_checkpoint=True)\n",
        "            else:\n",
        "                # Create a new study object for each fold\n",
        "                study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner(), sampler=optuna.samplers.TPESampler(seed=SEED),)\n",
        "                datamodule.setup(\"fit\", fold)\n",
        "                study.optimize(lambda trial: objective(trial, datamodule, EPOCHS, r, fold), n_trials=8)\n",
        "                print(f\"Best trial for fold {fold}: {study.best_trial.value}\")\n",
        "\n",
        "                # Retrain the model with the best hyperparameters\n",
        "                best_params = study.best_trial.params\n",
        "                model = GNNModel(in_channels=datamodule.num_node_features,\n",
        "                              out_channels=datamodule.num_classes,\n",
        "                              hidden_channels=best_params['hidden_channels'],\n",
        "                              dropout=best_params['dropout'])\n",
        "                datamodule.setup(\"fit\", fold, batch_size=best_params['batch_size'])\n",
        "                test_acc = retrain_and_evaluate(model, datamodule, EPOCHS, r, fold)\n",
        "            fold_performances.append(test_acc)\n",
        "\n",
        "        avg_performance = np.mean(fold_performances)\n",
        "        print(f\"Average performance for repetition {r}: {avg_performance}\")\n",
        "        overall_performances.append(avg_performance)\n",
        "\n",
        "    print(f\"Overall average performance: {np.mean(overall_performances)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sJYU7fBF3qU_",
        "outputId": "d456c66d-3421-4d39-ad64-a3616f93ca58"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7998064756393433    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7998064756393433     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-04-20 14:41:40,578] A new study created in memory with name: no-name-ec75abe6-3705-442d-baef-4f0151dac505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average performance for repetition 0: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-04-20 14:41:45,247] Trial 0 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 32, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:41:50,380] Trial 1 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:41:54,275] Trial 2 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:41:58,210] Trial 3 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:03,967] Trial 4 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:07,910] Trial 5 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 128, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:11,912] Trial 6 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:16,993] Trial 7 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for fold 0: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /content/gdrive/My Drive/ColabNotebooks/logs/NCI1_reps_2_folds_3_epochs_20_2023-04-20_15-48/checkpoints exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 0.00020317791495472193  \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  0.00020317791495472193   </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-04-20 14:42:21,720] A new study created in memory with name: no-name-33005e82-d212-4c26-b62f-ce235cc20ffc\n",
            "[I 2023-04-20 14:42:25,760] Trial 0 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 32, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:30,713] Trial 1 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:35,793] Trial 2 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:39,723] Trial 3 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:43,731] Trial 4 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:50,606] Trial 5 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 128, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:54,853] Trial 6 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:42:58,959] Trial 7 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /content/gdrive/My Drive/ColabNotebooks/logs/NCI1_reps_2_folds_3_epochs_20_2023-04-20_15-48/checkpoints exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for fold 1: 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 3.4875662095146254e-05  \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  3.4875662095146254e-05   </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-04-20 14:43:04,087] A new study created in memory with name: no-name-afac5481-d0e7-4106-9a26-86f51c20feb7\n",
            "[I 2023-04-20 14:43:08,799] Trial 0 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 32, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:43:13,015] Trial 1 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:43:17,563] Trial 2 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:43:22,444] Trial 3 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:43:26,457] Trial 4 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:43:30,379] Trial 5 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 128, 'dropout': 0.0}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:43:35,972] Trial 6 finished with value: 1.0 and parameters: {'hidden_channels': 16, 'batch_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "[I 2023-04-20 14:43:40,599] Trial 7 finished with value: 1.0 and parameters: {'hidden_channels': 32, 'batch_size': 128, 'dropout': 0.5}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /content/gdrive/My Drive/ColabNotebooks/logs/NCI1_reps_2_folds_3_epochs_20_2023-04-20_15-48/checkpoints exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for fold 2: 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 0.00013474954175762832  \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  0.00013474954175762832   </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average performance for repetition 1: 1.0\n",
            "Overall average performance: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorboard --logdir '/content/gdrive/My Drive/ColabNotebooks/'\n",
        "%tensorboard --logdir '{MAIN_DIR}'"
      ],
      "metadata": {
        "id": "t86u3fkaDp42"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPU/a942QaUlNnF8jruOAbz",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}